{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required modules\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re\n",
    "import heapq  \n",
    "from operator import itemgetter\n",
    "from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incase the data is from a wikipedia page use the following code to get the data from the site.\n",
    "#It return a string containing the information.\n",
    "def scrape_data(url_link):\n",
    "    scraped_data = urllib.request.urlopen(url_link)  \n",
    "    data = scraped_data.read()\n",
    "\n",
    "    parsed_data = bs.BeautifulSoup(data,'lxml')\n",
    "\n",
    "    paragraphs = parsed_data.find_all('p')\n",
    "\n",
    "    data_text = \"\"\n",
    "\n",
    "    for p in paragraphs:  \n",
    "        data_text += p.text\n",
    "    return data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the brackets and extra spaces present in the text.\n",
    "def remove_brackets_extraSpaces(raw_data):\n",
    "    raw_data = re.sub(r'\\[[0-9]*\\]', ' ', raw_data)  \n",
    "    raw_data = re.sub(r'\\s+', ' ', raw_data)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove special Character from the text. This was not included in the above function because \".\" is a special character and if we remove this then sentence tokenizing would return just one sentence due to absnce of \".\" in the raw data.\n",
    "def remove_specialChar(raw_data):\n",
    "    raw_data = re.sub('[^a-zA-Z]', ' ', raw_data )  \n",
    "    raw_data = re.sub(r'\\s+', ' ', raw_data)  \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentanize tokenize the processed text.\n",
    "def sentenceTokenize(paragraph):\n",
    "    return nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentanize tokenize the processed text.\n",
    "def wordTokenize(sent):\n",
    "    return nltk.word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    \n",
    "    sent1 = remove_specialChar(sent1)\n",
    "    sent2 = remove_specialChar(sent2)\n",
    "    \n",
    "    vocab = list(set(sent1 + sent2))\n",
    "    \n",
    "    vector1 = [0] * len(vocab)\n",
    "    vector2 = [0] * len(vocab)\n",
    "    \n",
    "    for w in sent1:\n",
    "        vector1[vocab.index(w)] += 1\n",
    "    \n",
    "    for w in sent2:\n",
    "        vector2[vocab.index(w)] += 1\n",
    "    \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix_data(sentenceList):\n",
    "    len_val = len(sentenceList)\n",
    "    sim_matrix = np.zeros([len_val, len_val])\n",
    "    for i in range(len_val):\n",
    "        for j in range(len_val):\n",
    "            if i != j:\n",
    "                sim_matrix[i][j] = sentence_similarity(sentenceList[i], sentenceList[j])\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    for i in range(len(sim_matrix)):\n",
    "        sim_matrix[i] /= sim_matrix[i].sum()\n",
    "    return sim_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PageRank(matrix, eps,d):\n",
    "    N = matrix.shape[1]\n",
    "    v = np.random.rand(N, 1)\n",
    "    v = v / np.linalg.norm(v, 1)\n",
    "    last_v = np.ones((N, 1), dtype=np.float32) * np.inf\n",
    "    M_hat = (d * matrix) + (((1 - d) / N) * np.ones((N, N), dtype=np.float32))\n",
    "    \n",
    "    while np.linalg.norm(v - last_v, 2) > eps:\n",
    "        last_v = v\n",
    "        v = np.matmul(M_hat, v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SummayFromURL(urlLink):\n",
    "    scrapped_data = scrape_data(urlLink)\n",
    "    brac_extra_space = remove_brackets_extraSpaces(scrapped_data)\n",
    "    sentences = sentenceTokenize(brac_extra_space)\n",
    "    similarityMatrix = similarity_matrix_data(sentences)\n",
    "    sentence_ranks = PageRank(similarityMatrix,1.0e-8,0.85)\n",
    "    rankedIndexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    Size_of_Summary = 10\n",
    "    selected_sentences = sorted(rankedIndexes[:Size_of_Summary])\n",
    "    return itemgetter(*selected_sentences)(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The AI field draws upon computer science, information engineering, mathematics, psychology, linguistics, philosophy, and many others.',\n",
       " 'AI often revolves around the use of algorithms.',\n",
       " 'A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.',\n",
       " 'Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal affect analysis (see multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.',\n",
       " 'Many learning algorithms use search algorithms based on optimization.',\n",
       " 'Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.',\n",
       " 'Some self-driving cars are not equipped with steering wheels or brake pedals, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.',\n",
       " 'It is possible to use AI to predict or generalize the behavior of customers from their digital footprints in order to target them with personalized promotions or build customer personas automatically.',\n",
       " 'Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem.',\n",
       " 'This issue, now known as \"robot rights\", is currently being considered by, for example, California\\'s Institute for the Future, although many critics believe that the discussion is premature.')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SummayFromURL('https://en.wikipedia.org/wiki/Artificial_intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
